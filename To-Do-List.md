I'll help you create a comprehensive training plan for your baseball pitch classification model. Let me first explore your workspace to understand what you have. 

Read [](file:///home/chishan/Projects/Baseball/youtube-videos)

Read [](file:///home/chishan/Projects/Baseball/mlb-youtube/README.md#1-1), lines 1 to 50

Read [](file:///home/chishan/Projects/Baseball/mlb-youtube/experiments)

Ran terminal command:  ls -1 /home/chishan/Projects/Baseball/youtube-videos | wc -l

Ran terminal command:  ls -1 /home/chishan/Projects/Baseball/youtube-videos | head -5 | xargs -I {} file "/home/chishan/Projects/Baseball/youtube-videos/{}"

Read [](file:///home/chishan/Projects/Baseball/mlb-youtube/experiments/train_segmented.py#1-1), lines 1 to 50

Read [](file:///home/chishan/Projects/Baseball/mlb-youtube/experiments/models.py#1-1), lines 1 to 80

Now let me create a comprehensive training plan for you: 

Created 9 todos

## Baseball Pitch Classification: Complete Training Plan

You currently have **~54 video clips** of baseball pitches with [strike] and [ball] labels. Here's your comprehensive training roadmap:

---

## ğŸ“‹ COMPLETE TO-DO LIST

### **Step 1: Data Preparation & Organization**
- **What**: Create proper dataset structure with train/validation/test splits
- **How**: 
  - Create folder structure: `data/strike/` and `data/ball/`
  - OR create annotation file (CSV/JSON) mapping video filenames to labels
  - Split: 70% train, 15% validation, 15% test
- **Why**: Organized data is essential for training

### **Step 2: Extract Frames from Videos**
- **What**: Convert videos to frame sequences
- **How**: Use OpenCV or FFmpeg
  - Extract at 10-30 FPS per video
  - Save as JPG/PNG images
- **Code**: `cv2.VideoCapture()` or `ffmpeg -i video.mp4 -vf fps=15 frames/%04d.jpg`

### **Step 3: YOLO Object Detection (Optional but Recommended)**
- **What**: Detect and localize pitcher, batter, ball in frames
- **How**: 
  - Use YOLOv8 or YOLOv5 pre-trained on COCO
  - Detect "person" class for pitcher/batter
  - Crop detected regions to focus model attention
- **Why**: Reduces background noise, improves accuracy
- **Tools**: `ultralytics` library for YOLOv8

### **Step 4: Feature Extraction with I3D/3D CNN**
- **What**: Extract spatio-temporal features from video clips
- **How**: 
  - Use I3D model (you have `pytorch-i3d/` folder)
  - Load pre-trained I3D weights (Kinetics-400)
  - Extract 1024-dim features per clip
- **Why**: Captures motion patterns distinguishing strikes from balls

### **Step 5: LSTM Temporal Modeling**
- **What**: Model temporal dependencies in frame sequences
- **How**: 
  - Feed I3D features into 2-3 layer LSTM (hidden size: 256-512)
  - Use bidirectional LSTM for better context
  - Add dropout (0.3-0.5) for regularization
- **Why**: Learns pitch trajectory and timing patterns

### **Step 6: Build Final Classification Model**
- **Architecture**: 
  ```
  Video Frames â†’ I3D (3D CNN) â†’ LSTM â†’ FC Layer â†’ [Strike/Ball]
  ```
- **Loss**: Binary Cross-Entropy or Categorical Cross-Entropy
- **Output**: 2 classes (Strike, Ball)

### **Step 7: Training Pipeline Setup**
- **Data Augmentation**: Random crop, horizontal flip, temporal jittering
- **Optimizer**: Adam (lr=1e-4) or SGD with momentum
- **Scheduler**: ReduceLROnPlateau or CosineAnnealing
- **Batch size**: 8-16 (depending on GPU memory)

### **Step 8: Train and Evaluate**
- **Metrics**: Accuracy, Precision, Recall, F1-score
- **Epochs**: 50-100 with early stopping (patience=10)
- **Validation**: Monitor on validation set every epoch
- **Testing**: Final evaluation on test set

### **Step 9: Model Optimization**
- Test on unseen videos
- Analyze misclassifications
- Fine-tune hyperparameters
- Consider ensemble or attention mechanisms

---

## ğŸ”¢ **MINIMUM DATASET SIZE**

**Your current 54 clips is INSUFFICIENT.** Here's what you need:

| Scenario | Minimum Clips | Recommended | Optimal |
|----------|--------------|-------------|---------|
| **Basic Model** | 200-300 per class | 500 per class | 1000+ per class |
| **Your Case (2 classes)** | **400-600 total** | **1000 total** | **2000+ total** |

**Why you need more:**
- Deep learning models (I3D + LSTM) require substantial data
- 54 clips â†’ ~38 train, ~8 val, ~8 test (too small!)
- Risk of severe overfitting
- Poor generalization to unseen pitches

**What to do:**
1. âœ… **Download more clips** (aim for 500-1000 total)
2. âœ… **Data augmentation** (flips, crops, speed variations) can help
3. âœ… **Transfer learning** from pre-trained I3D (reduces data requirements)
4. âœ… **Start simple**: Try training with current data but expect ~60-70% accuracy max

---

## ğŸš€ **RECOMMENDED APPROACH FOR YOUR 54 CLIPS**

Since you have limited data, use this **transfer learning approach**:

1. **Use pre-trained I3D** on Kinetics-400 (already includes sports actions)
2. **Freeze I3D layers**, only train LSTM + classifier
3. **Heavy augmentation**: temporal crop, speed perturbation, flips
4. **Simple LSTM**: 1-2 layers, not too deep
5. **Expect**: 60-75% accuracy with 54 clips, 85-95% with 500+ clips

---

Would you like me to help you:
1. Create the dataset organization script?
2. Set up the frame extraction pipeline?
3. Build the I3D + LSTM training script?
4. Download more video clips first?


***

User: As you can see in youtube-videos, I have downloaded some video-clips of pitching baseball, and I have tags [strike] [ball] for each clip. What should I do to train this model? Give me to-do-list, like which step yolo, which step LSTM , how to do, what to do      and tell me how many clips at least I need to train the model to be able to classify,

GitHub Copilot: I'll help you create a comprehensive training plan for your baseball pitch classification model. Let me first explore your workspace to understand what you have. 

Read [](file:///home/chishan/Projects/Baseball/youtube-videos)

Read [](file:///home/chishan/Projects/Baseball/mlb-youtube/README.md#1-1), lines 1 to 50

Read [](file:///home/chishan/Projects/Baseball/mlb-youtube/experiments)

Ran terminal command:  ls -1 /home/chishan/Projects/Baseball/youtube-videos | wc -l

Ran terminal command:  ls -1 /home/chishan/Projects/Baseball/youtube-videos | head -5 | xargs -I {} file "/home/chishan/Projects/Baseball/youtube-videos/{}"

Read [](file:///home/chishan/Projects/Baseball/mlb-youtube/experiments/train_segmented.py#1-1), lines 1 to 50

Read [](file:///home/chishan/Projects/Baseball/mlb-youtube/experiments/models.py#1-1), lines 1 to 80

Now let me create a comprehensive training plan for you: 

Created 9 todos

## Baseball Pitch Classification: Complete Training Plan

You currently have **~54 video clips** of baseball pitches with [strike] and [ball] labels. Here's your comprehensive training roadmap:

---

## ğŸ“‹ COMPLETE TO-DO LIST

### **Step 1: Data Preparation & Organization**
- **What**: Create proper dataset structure with train/validation/test splits
- **How**: 
  - Create folder structure: `data/strike/` and `data/ball/`
  - OR create annotation file (CSV/JSON) mapping video filenames to labels
  - Split: 70% train, 15% validation, 15% test
- **Why**: Organized data is essential for training

### **Step 2: Extract Frames from Videos**
- **What**: Convert videos to frame sequences
- **How**: Use OpenCV or FFmpeg
  - Extract at 10-30 FPS per video
  - Save as JPG/PNG images
- **Code**: `cv2.VideoCapture()` or `ffmpeg -i video.mp4 -vf fps=15 frames/%04d.jpg`

### **Step 3: YOLO Object Detection (Optional but Recommended)**
- **What**: Detect and localize pitcher, batter, ball in frames
- **How**: 
  - Use YOLOv8 or YOLOv5 pre-trained on COCO
  - Detect "person" class for pitcher/batter
  - Crop detected regions to focus model attention
- **Why**: Reduces background noise, improves accuracy
- **Tools**: `ultralytics` library for YOLOv8

### **Step 4: Feature Extraction with I3D/3D CNN**
- **What**: Extract spatio-temporal features from video clips
- **How**: 
  - Use I3D model (you have `pytorch-i3d/` folder)
  - Load pre-trained I3D weights (Kinetics-400)
  - Extract 1024-dim features per clip
- **Why**: Captures motion patterns distinguishing strikes from balls

### **Step 5: LSTM Temporal Modeling**
- **What**: Model temporal dependencies in frame sequences
- **How**: 
  - Feed I3D features into 2-3 layer LSTM (hidden size: 256-512)
  - Use bidirectional LSTM for better context
  - Add dropout (0.3-0.5) for regularization
- **Why**: Learns pitch trajectory and timing patterns

### **Step 6: Build Final Classification Model**
- **Architecture**: 
  ```
  Video Frames â†’ I3D (3D CNN) â†’ LSTM â†’ FC Layer â†’ [Strike/Ball]
  ```
- **Loss**: Binary Cross-Entropy or Categorical Cross-Entropy
- **Output**: 2 classes (Strike, Ball)

### **Step 7: Training Pipeline Setup**
- **Data Augmentation**: Random crop, horizontal flip, temporal jittering
- **Optimizer**: Adam (lr=1e-4) or SGD with momentum
- **Scheduler**: ReduceLROnPlateau or CosineAnnealing
- **Batch size**: 8-16 (depending on GPU memory)

### **Step 8: Train and Evaluate**
- **Metrics**: Accuracy, Precision, Recall, F1-score
- **Epochs**: 50-100 with early stopping (patience=10)
- **Validation**: Monitor on validation set every epoch
- **Testing**: Final evaluation on test set

### **Step 9: Model Optimization**
- Test on unseen videos
- Analyze misclassifications
- Fine-tune hyperparameters
- Consider ensemble or attention mechanisms

---

## ğŸ”¢ **MINIMUM DATASET SIZE**

**Your current 54 clips is INSUFFICIENT.** Here's what you need:

| Scenario | Minimum Clips | Recommended | Optimal |
|----------|--------------|-------------|---------|
| **Basic Model** | 200-300 per class | 500 per class | 1000+ per class |
| **Your Case (2 classes)** | **400-600 total** | **1000 total** | **2000+ total** |

**Why you need more:**
- Deep learning models (I3D + LSTM) require substantial data
- 54 clips â†’ ~38 train, ~8 val, ~8 test (too small!)
- Risk of severe overfitting
- Poor generalization to unseen pitches

**What to do:**
1. âœ… **Download more clips** (aim for 500-1000 total)
2. âœ… **Data augmentation** (flips, crops, speed variations) can help
3. âœ… **Transfer learning** from pre-trained I3D (reduces data requirements)
4. âœ… **Start simple**: Try training with current data but expect ~60-70% accuracy max

---

## ğŸš€ **RECOMMENDED APPROACH FOR YOUR 54 CLIPS**

Since you have limited data, use this **transfer learning approach**:

1. **Use pre-trained I3D** on Kinetics-400 (already includes sports actions)
2. **Freeze I3D layers**, only train LSTM + classifier
3. **Heavy augmentation**: temporal crop, speed perturbation, flips
4. **Simple LSTM**: 1-2 layers, not too deep
5. **Expect**: 60-75% accuracy with 54 clips, 85-95% with 500+ clips

---

Would you like me to help you:
1. Create the dataset organization script?
2. Set up the frame extraction pipeline?
3. Build the I3D + LSTM training script?
4. Download more video clips first?

User: Assume you are an expert at MachinLearning. Now I've downloaded about 1000 7-second  clips(in ./dataset/videos) and all of them are tagged "strike" or "ball" in ./dataset/pitchcalls/labels.csv . But questions exist. First, many of the clips have irrevelant scenes in clips, like the scene of th audience. Second, I don't know what to do next: I first need to train the model to pay attention to the pitcher,hitter and the ball? (and the box of the strikezoneï¼Œwhich appear in many clips)

GitHub Copilot: Read [](file:///home/chishan/Projects/Baseball/dataset/pitchcalls/labels.csv#1-1), lines 1 to 20

Ran terminal command:  wc -l /home/chishan/Projects/Baseball/dataset/pitchcalls/labels.csv

Ran terminal command:  ls /home/chishan/Projects/Baseball/dataset/videos | head -5

Ran terminal command:  ls /home/chishan/Projects/Baseball/dataset/videos | wc -l

Ran terminal command:  cd /home/chishan/Projects/Baseball/dataset/pitchcalls && awk -F',' 'NR>1 {print $3}' labels.csv | sort | uniq -c

Perfect! You have **983 clips** with **610 strikes and 373 balls**. This is a much better dataset size. As an ML expert, let me answer your critical questions:

---

## ğŸ¯ **EXPERT ANSWER: YOU DON'T NEED YOLO/DETECTION FIRST**

**Short answer**: **NO**, you should **NOT** train a separate attention/detection model first. Go directly to end-to-end classification.

### **Why NOT use YOLO/Detection pipeline:**

1. **3D CNNs (I3D) already learn spatial attention** - They automatically focus on relevant regions through convolutional filters
2. **End-to-end learning is more effective** - The model learns which regions matter for strike/ball classification
3. **Detection adds complexity & error propagation** - If YOLO misses the ball/pitcher, your entire pipeline fails
4. **You'd need labeled bounding boxes** - Requires manual annotation of 1000+ videos (weeks of work)
5. **Irrelevant scenes (audience) won't hurt** - Modern CNNs are robust to background noise

### **What Modern Research Shows:**

The state-of-the-art approach for video classification is:
- **Raw video â†’ 3D CNN/I3D â†’ Temporal model â†’ Classification**
- No detection needed! Models learn spatial+temporal patterns end-to-end

---

## ğŸš€ **RECOMMENDED PIPELINE (Expert Approach)** 

Created 10 todos

## ğŸ“Š **YOUR UPDATED TO-DO LIST**

### **Phase 1: Data Preparation (1-2 hours)**
1. âœ… **Setup Project Structure** - Create folders, install dependencies
2. âœ… **Create Dataset Split** - 70/15/15 train/val/test split with stratification
3. âœ… **Build Video DataLoader** - PyTorch Dataset class with augmentation

### **Phase 2: Model Development (2-4 hours)**
4. âœ… **Load Pre-trained I3D** - Use transfer learning from Kinetics-400
5. âœ… **Build Training Pipeline** - Optimizer, loss, scheduler, checkpointing
6. âœ… **Train Initial Model** - **START HERE** with simple I3D â†’ FC baseline

### **Phase 3: Enhancement (2-3 hours)**
7. âœ… **Add Temporal Modeling** - LSTM/Transformer after baseline works
8. âœ… **Handle Class Imbalance** - Weighted loss for 610 strikes vs 373 balls
9. âœ… **Evaluate and Fine-tune** - Test set evaluation, error analysis

### **Phase 4: Deployment**
10. âœ… **

User: @agent Try Again

GitHub Copilot: ## ğŸ“‹ **UPDATED EXPERT TO-DO LIST**

### **Phase 1: Baseline (Start Simple!)**

**Step 1: Setup Project Structure**
```bash
mkdir -p models checkpoints data/{train,val,test}
pip install torch torchvision opencv-python pandas scikit-learn matplotlib
```

**Step 2: Data Split Script**
- Split 983 clips: 70% train (688), 15% val (147), 15% test (148)
- Use stratification to keep 610:373 ratio in each split

**Step 3: Video DataLoader**
- Sample 16 frames uniformly from each 7-sec clip
- Resize to 224Ã—224
- Apply augmentation: random crop, flip, color jitter
- Normalize with ImageNet stats

**Step 4: Simple I3D Baseline** â­ **START HERE**
- Use `torchvision.models.video.r3d_18` (pre-trained)
- Replace final layer: 512 â†’ 2 classes
- Direct pipeline: **Video â†’ I3D â†’ Softmax â†’ [Strike/Ball]**
- Train for 30 epochs
- **Expected: 75-85% accuracy**

**Step 5: Add Temporal Modeling** (Optional, if baseline works)
- Extract I3D features â†’ Feed to LSTM â†’ Classify
- **Expected: 85-90% accuracy**

---

## ğŸ¯ **ADDRESSING YOUR CONCERNS**

### **Problem 1: Irrelevant scenes (audience, replays)**
**Solution**: 
- âœ… **Don't worry!** 3D CNNs learn to ignore irrelevant regions automatically
- âœ… The model will focus on motion patterns (pitcher windup, ball trajectory)
- âœ… If needed, add **temporal cropping**: extract middle 3-4 seconds only
- âœ… Data augmentation handles scene variations

### **Problem 2: Should I train attention to pitcher/hitter/ball/strike zone?**
**Solution**:
- âŒ **NO explicit attention training needed**
- âœ… I3D has **implicit attention** through conv filters
- âœ… If you want explicit attention (advanced): Use **Attention-based 3D CNN** or **Grad-CAM** visualization AFTER training to verify what model looks at
- âœ… Strike zone box is actually **helpful visual cue** - keep it!

---

## ğŸ’¡ **EXPERT RECOMMENDATION**

### **Your Best Approach (Proven in Research):**

```
Raw 7-sec videos â†’ Sample 16 frames â†’ I3D/R3D â†’ Global Average Pool â†’ FC â†’ [Strike/Ball]
```

**Architecture**:
1. **Input**: 16 frames Ã— 224Ã—224 Ã— 3 (RGB)
2. **Backbone**: R3D-18 or I3D (pre-trained on Kinetics)
3. **Pooling**: Temporal + Spatial pooling
4. **Classifier**: FC(512 â†’ 2)
5. **Loss**: Weighted CrossEntropy (handle 610:373 imbalance)

**Why this works**:
- âœ… Proven for sports action recognition
- âœ… No manual attention needed
- âœ… Handles noisy backgrounds
- âœ… Learns both spatial (pitcher pose) + temporal (ball trajectory) features
- âœ… End-to-end learnable

---

## ğŸ“Š **EXPECTED RESULTS WITH YOUR DATA**

| Approach | Accuracy | Training Time | Complexity |
|----------|----------|---------------|------------|
| **I3D only (baseline)** | **75-85%** | 2-4 hours | Simple â­ |
| I3D + LSTM | 85-90% | 4-6 hours | Medium |
| I3D + Attention | 85-92% | 5-8 hours | Complex |
| Ensemble (I3D+R3D) | 90-95% | 8-12 hours | Advanced |

With **983 clips**, you have enough data for 85%+ accuracy!

---

## ğŸ› ï¸ **NEXT STEPS - WHAT TO DO NOW**

Would you like me to:

1. âœ… **Create the data split script** (train/val/test split)?
2. âœ… **Build the PyTorch DataLoader** for video loading?
3. âœ… **Write the training script** with I3D baseline?
4. âœ… **Set up the full pipeline end-to-end**?

**My recommendation**: Let me create **option 4** - a complete, working training pipeline you can run immediately. Just say "yes" and I'll build it!